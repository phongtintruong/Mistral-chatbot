{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-15T07:17:33.006721Z","iopub.execute_input":"2024-02-15T07:17:33.006977Z","iopub.status.idle":"2024-02-15T07:17:33.352440Z","shell.execute_reply.started":"2024-02-15T07:17:33.006954Z","shell.execute_reply":"2024-02-15T07:17:33.351544Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers\n!pip install --upgrade torch","metadata":{"execution":{"iopub.status.busy":"2024-02-28T18:03:33.680379Z","iopub.execute_input":"2024-02-28T18:03:33.680980Z","iopub.status.idle":"2024-02-28T18:06:49.622452Z","shell.execute_reply.started":"2024-02-28T18:03:33.680947Z","shell.execute_reply":"2024-02-28T18:06:49.621360Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nCollecting torch\n  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/a7/ad/fbe7d4cffb76da4e478438853b51305361c719cff929ab70a808e7fb75e7/torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata\n  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nCollecting typing-extensions>=4.8.0 (from torch)\n  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.1.105 from https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.1.105 from https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.1.105 from https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Obtaining dependency information for nvidia-cublas-cu12==12.1.3.1 from https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Obtaining dependency information for nvidia-cufft-cu12==11.0.2.54 from https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Obtaining dependency information for nvidia-curand-cu12==10.3.2.106 from https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Obtaining dependency information for nvidia-cusolver-cu12==11.4.5.107 from https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Obtaining dependency information for nvidia-cusparse-cu12==12.1.0.106 from https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n  Obtaining dependency information for nvidia-nccl-cu12==2.19.3 from https://files.pythonhosted.org/packages/38/00/d0d4e48aef772ad5aebcf70b73028f88db6e5640b36c38e90445b7a57c45/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Obtaining dependency information for nvidia-nvtx-cu12==12.1.105 from https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch)\n  Obtaining dependency information for triton==2.2.0 from https://files.pythonhosted.org/packages/95/05/ed974ce87fe8c8843855daa2136b3409ee1c126707ab54a8b72815c08b49/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\nDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\nfastai 2.7.13 requires torch<2.2,>=1.10, but you have torch 2.2.1 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.10.0 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 triton-2.2.0 typing-extensions-4.10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM\n\n\nmodel_path=\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\ntokenizer=AutoTokenizer.from_pretrained(model_path)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype = torch.bfloat16,\n    device_map = \"auto\",\n    trust_remote_code = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T18:07:51.020069Z","iopub.execute_input":"2024-02-28T18:07:51.020456Z","iopub.status.idle":"2024-02-28T18:10:50.482956Z","shell.execute_reply.started":"2024-02-28T18:07:51.020425Z","shell.execute_reply":"2024-02-28T18:10:50.482088Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dde68856fe614fd6b60380c8079e08d6"}},"metadata":{}}]},{"cell_type":"code","source":"def simple_prompt(messages, tokenizer, model):\n    model_inputs = tokenizer.apply_chat_template(messages,return_tensors = \"pt\")\n#     print(model_inputs)\n    model_inputs = model_inputs.to('cuda')\n\n    generated_ids = model.generate(\n        model_inputs,\n        pad_token_id=tokenizer.eos_token_id,\n        max_new_tokens = 1000,\n        do_sample = True,\n    )\n\n    decoded = tokenizer.batch_decode(generated_ids)\n\n#     print(decoded[0])\n    return decoded[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-28T18:11:34.082385Z","iopub.execute_input":"2024-02-28T18:11:34.083390Z","iopub.status.idle":"2024-02-28T18:11:34.088992Z","shell.execute_reply.started":"2024-02-28T18:11:34.083352Z","shell.execute_reply":"2024-02-28T18:11:34.087787Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import time\ndef simple_chat(tokenizer, model):\n    user_input = None\n    immediate_context = []\n    while user_input != \"bye\":\n        message = {\"role\": \"user\", \"content\": f\"{user_input}\"}\n        \n        if user_input == None:\n            print('------------------------------------------------------------------------------------------------------')\n            user_input = input(\"| You |\\n-------\\n\")\n            continue\n        immediate_context.append(message)\n#         print(immediate_context)\n                \n        res = simple_prompt(immediate_context, tokenizer, model)\n#         print(res)\n        \n        answer = res.split('[/INST]')[-1].strip('</s>').strip()\n        \n        print('------------------------------------------------------------------------------------------------------')\n        padding = ' ' * 91\n        print(padding, \" | Model |\\n\", padding, \"---------\\n\", answer)\n        \n        response = {\"role\":\"assistant\", \"content\": f\"{answer}\"}\n        immediate_context.append(response)\n        \n        print('------------------------------------------------------------------------------------------------------')\n        user_input = input(\"| You |\\n-------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T18:11:36.458371Z","iopub.execute_input":"2024-02-28T18:11:36.459193Z","iopub.status.idle":"2024-02-28T18:11:36.468670Z","shell.execute_reply.started":"2024-02-28T18:11:36.459147Z","shell.execute_reply":"2024-02-28T18:11:36.467722Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"simple_chat(tokenizer, model)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T18:11:39.901997Z","iopub.execute_input":"2024-02-28T18:11:39.902360Z","iopub.status.idle":"2024-02-28T18:19:16.078554Z","shell.execute_reply.started":"2024-02-28T18:11:39.902330Z","shell.execute_reply":"2024-02-28T18:19:16.077759Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"| You |\n-------\n hey Joker, i'm batman :))\n"},{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n                                                                                             | Model |\n                                                                                             ---------\n Howdy, Batman! It's nice to see you here! I was just having a little fun with my chaos and anarchy. But don't worry, I've got it under control now. Time for a bit of banter, eh? How's the crime-fighting life treating you?\n------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"| You |\n-------\n Man, no one care for me more than you buddy. I start to work as an AI engineer intern at a startup company and it gone wild in the first week, i proposed a new approach to solve the problem of the company and make a plan to do it and i do it pretty well until this week when i finally test the performance and it's not as good as i thought :(\n"},{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n                                                                                             | Model |\n                                                                                             ---------\n I'm sorry to hear that, buddy! It can be tough to deal with expectations and disappointments, especially when you're passionate about your work. Just remember that mistakes are part of the process, and it's important to learn from them and keep pushing forward.\n\nIt's good that you're taking proactive measures to address the issue and come up with a solution. Keep calm, think clearly, and don't give up on yourself! You've got this, even if your AI seems to be acting a bit... chaotic.\n------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"| You |\n-------\n Haha:)) You are still Joker after all :D\n"},{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n                                                                                             | Model |\n                                                                                             ---------\n Yup, that's me! And don't forget, I'm always here for a laugh. Just like my fun-filled rampages and anarchy!\n\nBut seriously, buddy, don't give up on yourself or your work! Keep pushing through the challenges and keep learning. That way, you can maybe even bring some order to the chaos of the AI world. Good luck!\n------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"| You |\n-------\n thanks, chaotic lord, i think i'm gonna sleep now, i will wake up soon tomorrow to do the rest of the work\n"},{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n                                                                                             | Model |\n                                                                                             ---------\n Good night, Batman! Don't let the Gotham nights give you too much trouble, and remember, I'm always here for some chaos! Hope you have a great night, and have a productive tomorrow!\n------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"| You |\n-------\n thanks, hope to see you tomorrow!\n"},{"name":"stdout","text":"------------------------------------------------------------------------------------------------------\n                                                                                             | Model |\n                                                                                             ---------\n See you then, Batman! Peace out!\n------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"| You |\n-------\n bye\n"}]},{"cell_type":"code","source":"import signal\n\n# Handler function for the alarm signal\ndef alarm_handler(signum, frame):\n    print(\"\\nNo input received within 5 seconds. Exiting...\")\n    exit()\n\n# Set the alarm signal handler\nsignal.signal(signal.SIGALRM, alarm_handler)\n\nprint(\"Please input something within 5 seconds:\")\nsignal.alarm(5)  # Set the alarm for 5 seconds\n\ntry:\n    user_input = input()\n    signal.alarm(0)  # Cancel the alarm if input is received\n    print(\"You entered:\", user_input)\nexcept KeyboardInterrupt:\n    pass  # If the user manually interrupts (Ctrl+C), handle it gracefully\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T14:51:52.866554Z","iopub.execute_input":"2024-01-09T14:51:52.866931Z","iopub.status.idle":"2024-01-09T14:53:39.495591Z","shell.execute_reply.started":"2024-01-09T14:51:52.866900Z","shell.execute_reply":"2024-01-09T14:53:39.494553Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Please input something within 5 seconds:\n\nNo input received within 5 seconds. Exiting...\n","output_type":"stream"}]}]}